#Get dataset about house price in boston
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error
from sklearn.metrics import make_scorer

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor 
from sklearn.ensemble import GradientBoostingRegressor 
import  matplotlib.pyplot as plt

X,Y = load_boston(return_X_y=True)
#model = RandomForestRegressor()
model2=GradientBoostingRegressor()

#settings for the algorithm 
# n_jobs = -1 automatically parallelize 
hyperSearch = GridSearchCV(model2,param_grid={
    'n_estimators':[10,50,100],'max_features':[0.25,0.5,0.75]}
                           ,scoring=make_scorer(
                               mean_squared_error,
                               greater_is_better=False)
                           ,cv=3
                           ,n_jobs=-1
                           ,verbose=True)  

xTrain, xTest, yTrain, yTest = train_test_split(X,Y,test_size=0.1)
hyperSearch.fit(xTrain,yTrain)

bestModel = hyperSearch.best_estimator_
mean_squared_error(yTrain,bestModel.predict(xTrain))
bestModel
mean_squared_error(yTest,bestModel.predict(xTest))
plt.scatter(yTest,bestModel.predict(xTest))
plt.plot(yTest,yTest,'-r')
plt.xlabel('True Price')
plt.ylabel('Predicted Price')
plt.title('Residuals plot')


plt.hist(yTrain)
plt.title('Training Distribution')
plt.show()
plt.hist(yTest)
plt.title('Testing Distribution')
plt.show()


Then, you can do in the another window for python;

from joblib import load
from sklearn.datasets import load_boston
X,Y = load_boston(return_X_y=1)
theModel = load('LeModel')
theModel.predict(X)
from joblib import dump, load
dumped = dump(bestModel, 'LeModel')
